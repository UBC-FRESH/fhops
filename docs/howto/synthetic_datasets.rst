Synthetic Datasets
==================

FHOPS ships with a small library of synthetic scenarios that mirror the structure of our teaching
and benchmarking datasets. Each bundle is a self-contained directory under
``examples/synthetic/`` with a ``scenario.yaml`` and the CSV tables that back it. The bundles are
generated by :func:`fhops.scenario.synthetic.generate_random_dataset` so they follow the same data
contract validation as user-supplied scenarios.

Reference bundles
-----------------

Three ready-to-use bundles are versioned with the project. The table below summarises their scale;
see ``examples/synthetic/metadata.yaml`` for the full metadata (random seeds, blackout windows, and
shift calendars).

.. list-table:: Reference bundle statistics
   :header-rows: 1
   :widths: 15 12 12 12 12 12 25

   * - Bundle
     - Blocks
     - Machines
     - Landings
     - Days
     - Shifts/day
     - Seed / Notes
   * - ``small``
     - 4
     - 2
     - 1
     - 6
     - 1
     - Seed ``101`` — single-shift schedule without blackouts.
   * - ``medium``
     - 8
     - 4
     - 2
     - 12
     - 1
     - Seed ``202`` — includes short blackout windows for downtime realism.
   * - ``large``
     - 16
     - 6
     - 3
     - 18
     - 2
     - Seed ``303`` — two-shift calendar with extended blackout periods.

The example README (``examples/synthetic/README.md``) gives a short overview and links back to the
metadata file if you need exact counts or blackout windows for reproducibility.

CLI walkthrough
---------------

Use the bundles anywhere a normal scenario is accepted. Typical entry points:

* Validate the data contract:

  .. code-block:: bash

     fhops validate examples/synthetic/medium/scenario.yaml

* Run the simulated annealing baseline and capture the schedule:

  .. code-block:: bash

     fhops solve-heur examples/synthetic/medium/scenario.yaml --out /tmp/medium_sa.csv --seed 99

* Generate shift/day summaries with KPI exports:

  .. code-block:: bash

     fhops eval playback --scenario examples/synthetic/medium/scenario.yaml \\
                         --assignments /tmp/medium_sa.csv \\
                         --shift-out /tmp/medium_shift.csv \\
                         --day-out /tmp/medium_day.csv \\
                         --summary-md /tmp/medium_playback.md

* Benchmark solver presets across the reference datasets:

  .. code-block:: bash

     fhops bench suite --scenario examples/synthetic/large/scenario.yaml \\
                       --operator-preset explore \\
                       --out-dir tmp/bench_synth_large

These commands reuse the same CLI surfaces already documented in :doc:`evaluation` and
:doc:`../reference/cli`, but the synthetic bundles keep the inputs lightweight enough for quick
iteration and teaching exercises.

Regenerating bundles
--------------------

The generator can be scripted to refresh or extend the library. Seeds are recorded in
``metadata.yaml`` so you can rebuild a bundle exactly:

.. code-block:: python

   from pathlib import Path

   from fhops.scenario.synthetic import SyntheticDatasetConfig, generate_random_dataset

   config = SyntheticDatasetConfig(
       name="synthetic-medium",
       num_blocks=(8, 10),
       num_days=12,
       num_machines=4,
       num_landings=2,
       shifts_per_day=1,
       shift_hours=(9.5, 10.5),
   )

   bundle = generate_random_dataset(config, seed=202)
   bundle.write(Path("examples/synthetic/medium"))

You can customise the config ranges (blocks, production rates, blackout probabilities, and role
assignments) or pass a ``systems`` dictionary to incorporate predefined harvest system templates via
``generate_random_dataset(..., systems=default_system_registry())``.

For small teaching examples, the deterministic helper :func:`fhops.scenario.synthetic.generate_basic`
creates a scenario with a single landing and no randomisation; for system-aware datasets use
:func:`fhops.scenario.synthetic.generate_with_systems`.

Where to go next
----------------

* ``tests/test_synthetic_dataset.py`` illustrates the expected invariants and is a good starting
  point if you extend the generator.
* The benchmarking how-to (:doc:`benchmarks`) explains how to wire new scenarios into the benchmarking
  harness once you are happy with the dataset characteristics.
