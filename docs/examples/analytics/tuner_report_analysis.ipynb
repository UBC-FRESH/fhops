{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4698bafe",
   "metadata": {},
   "source": [
    "# Telemetry Report Comparison\n",
    "\n",
    "This notebook demonstrates how to load multiple telemetry tuning reports,\n",
    "merge them, and visualize objective improvements using the helper\n",
    "`analyze_tuner_reports.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee331d4e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll reuse the CLI helpers that generated the latest `demo_tuner_report.csv`\n",
    "under `docs/examples/analytics/data/tuner_reports/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44107d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import altair as alt\n",
    "except ImportError:\n",
    "    alt = None\n",
    "\n",
    "DATA_DIR = Path('docs/examples/analytics/data/tuner_reports')\n",
    "BASELINE = DATA_DIR / 'demo_tuner_report.csv'\n",
    "EXPERIMENT = DATA_DIR / 'demo_tuner_report.csv'  # replace with new report as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643aa5c5",
   "metadata": {},
   "source": [
    "## Load Reports\n",
    "We can either call the helper script via `subprocess` or load the CSVs directly\n",
    "for ad-hoc comparisons.\n",
    "\n",
    "\n",
    "> CI executes with a lightweight telemetry sweep, so the required CSV files are generated automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BASELINE.exists() or not EXPERIMENT.exists():\n",
    "    print('Telemetry reports not found. Skipping comparison; run the telemetry sweep as described in docs/howto/telemetry_tuning.rst.')\n",
    "    merged = pd.DataFrame()\n",
    "else:\n",
    "    baseline = pd.read_csv(BASELINE)\n",
    "    experiment = pd.read_csv(EXPERIMENT)\n",
    "    baseline['label'] = 'baseline'\n",
    "    experiment['label'] = 'experiment'\n",
    "    merged = pd.concat([baseline, experiment])\n",
    "    display(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5e69e",
   "metadata": {},
   "source": [
    "## Visualize Best Objectives\n",
    "Plot the best objective per algorithm to see improvements/deltas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938031d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if alt is None:\n",
    "    display('Altair not installed; install `altair` to render the chart.')\n",
    "else:\n",
    "    chart = (\n",
    "        alt.Chart(merged)\n",
    "        .mark_line(point=True)\n",
    "        .encode(\n",
    "            x='label:N',\n",
    "            y='best_objective:Q',\n",
    "            color='algorithm:N',\n",
    "            column='scenario:N'\n",
    "        )\n",
    "    )\n",
    "    chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50867e",
   "metadata": {},
   "source": [
    "## Delta Table\n",
    "Join baseline and experiment to compute deltas using pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged.empty:\n",
    "    comparison = pd.DataFrame()\n",
    "    print('No comparison data loaded.')\n",
    "else:\n",
    "    comparison = baseline.merge(\n",
    "        experiment, on=['algorithm', 'scenario'], suffixes=('_baseline', '_experiment')\n",
    "    )\n",
    "    comparison['best_delta'] = (\n",
    "        comparison['best_objective_experiment'] - comparison['best_objective_baseline']\n",
    "    )\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231ab1f",
   "metadata": {},
   "source": [
    "Use this notebook as a starting point for richer analytics (e.g., trendlines\n",
    "across multiple nightly reports)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
