% Section 4: Impact
\section{Impact}
\label{sec:impact}

\subsection{Adoption and governance}
FHOPS is already the operational backbone for the UBC FRESH programme’s harvest-planning experiments: the same repository powers the public Sphinx documentation, the MASc workflow notebooks being prepared by Jaffray et~al., and the SoftwareX automation described here. Every feature showcased in Sections~\ref{sec:software-description} and \ref{sec:illustrative-example} ships on the main branch and will be tagged as part of the upcoming \texttt{v1.0.0-beta1} release, ensuring reviewers can fetch the exact code snapshot referenced in this manuscript. Governance stays intentionally lightweight—issues and pull requests are triaged by the maintainer listed in the metadata, and the reproducibility pipeline (\texttt{make manuscript-benchmarks}) doubles as the weekly regression suite for FHOPS’ documentation and demo datasets.

\subsection{Reproducibility and reuse}
Automation is the primary impact driver. Readers can run \texttt{make manuscript-benchmarks} (or the documented CLI equivalents) to regenerate every dataset, solver run, tuning study, playback analysis, costing table, and figure. Each run appends a signed entry to \texttt{docs/softwarex/assets/benchmark\_runs.log} containing the UTC timestamp, commit hash, total duration, and SHA-256 digest of the regenerated assets. Because the manuscript now \emph{inputs} the generated tables/figures directly (Tables~\ref{tab:benchmarks}--\ref{tab:tuning}, Figures~\ref{fig:playback}--\ref{fig:scaling}), reviewers can cross-check every narrative claim against a concrete CSV/JSON artefact. This same pipeline feeds the FHOPS documentation site, so production users and reviewers stay aligned on terminology, default solver presets, and telemetry formats.

\subsection{Future deployments}
The SoftwareX paper deliberately scopes itself to the open-source platform: scenario contracts, solver stack, telemetry, and automation. Upcoming BC case studies (community-forest CTL, skyline-with-tethered processors, salvage programmes) will reuse the same CLI/API workflow, benchmark presets, and playback harness documented here so they can publish datasets and telemetry without duplicating tooling narrative. As those deployments are released, the artefacts will slot directly into the FHOPS reference catalogue, giving practitioners real-world validation data that share the same provenance checksums as the manuscript assets. Until then FHOPS acts as the canonical release pipeline: once \texttt{v1.0.0-beta1} is tagged alongside this submission, downstream teams will be able to cite the identical automation scripts, checkpoint hashes, and dataset manifests when they begin their own deployments.
