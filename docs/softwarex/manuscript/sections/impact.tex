% Section 4: Impact
\section{Impact}
\label{sec:impact}

\subsection{Addressing the gaps identified by Jaffray et\,al.}
The 2025 systematic review stressed that operational-planning studies rarely publish reusable code, almost never share telemetry, and seldom document how to extend their models beyond the initial case study. FHOPS answers those gaps directly: it formalises the scenario contract (blocks, machines, mobilisation, shifts), exposes the solver/evaluation stack via CLI and Python APIs, and version-controls every dataset used in the benchmarks. Because the schema mirrors what BC practitioners already use, the same bundle can serve research, regulatory, and community-forest planning needs without custom adapters.

\subsection{Reproducible benchmarking infrastructure}
Running \texttt{make manuscript-benchmarks} regenerates every dataset inspection, solver run, tuning study, playback analysis, costing demo, and synthetic scaling sweep. Each invocation appends a signed record (UTC timestamp, commit, SHA-256 hash) to \texttt{docs/softwarex/assets/benchmark\_runs.log}, and the resulting artefacts live under \texttt{docs/softwarex/assets/data/**}. Tables~\ref{tab:benchmarks}--\ref{tab:tuning} and Figures~\ref{fig:playback}--\ref{fig:scaling} are ingested directly from those CSV/JSON files, so other researchers—and future case-study authors—can cross-check every number against a concrete asset path. The same scripts power the public FHOPS documentation, keeping narrative, CLI help, and manuscript perfectly aligned.

\subsection{Project maturity and near-term scope}
FHOPS is a brand-new public release: \texttt{v1.0.0-beta1} is the first tag outside the UBC FRESH lab, and no external deployments have been run yet. Publishing now locks in the reproducible tooling so that the upcoming BC case studies (documented in the roadmap and Jaffray et\,al.’s review) inherit a stable CLI/API workflow rather than ad hoc notebooks. Internal pilots and solver validation exercises informed the architecture, but we wait to claim adoption metrics until partners replicate it on real tenures. Near-term work centers on (i) proving rolling-horizon re-solving on large84 so multi-system horizons can enter the reproducibility log, (ii) shipping the dataset-inspection CLI to keep reference bundles realistic as new data arrives, and (iii) documenting the BC deployment plan (ground-based CTL today, skyline/helicopter/salvage as those datasets thaw) so downstream manuscripts cite a single artefact while domain-specific analyses proceed independently.

\subsection{Extensible deployments beyond British Columbia}
The current reference ladder (tiny7, small21, med42) intentionally focuses on BC ground-based cut-to-length systems because those bundles are public, reproducible, and already exercised by partner tenures. Large84 introduces two concurrent systems and will only enter the manuscript once rolling-horizon MIP runs are reproducible; until then it remains a roadmap stress case. This scope does not limit FHOPS to BC. The data contract, productivity/cost helper modules, and CLI plumbing were designed to accept new machine catalogues, block schemas, and currency conventions without touching solver code. The release already imports regression models and cost tables from FPInnovations and AFORA/ALPACA, proving that adding non-BC references is a configuration exercise rather than a refactor. As demand emerges elsewhere—other Canadian provinces or international tenure systems—we can extend FHOPS with alternate units, currencies, and machine defaults via the same automation described here.

\subsection{Path to broader community uptake}
FHOPS ships under the MIT licence, accepts contributions via the public repository, and uses the same automation described above for everyday testing. That governance model makes it easy for regulators, Indigenous governments, or cooperatives to audit and extend the tooling without waiting for bespoke consulting engagements. When new datasets or helper modules land, contributors can add them through the shared CLI/API and immediately benefit from the reproducibility guarantees. In short, FHOPS provides the “common infrastructure” the literature has been missing: a maintained, auditable foundation that lowers the cost of publishing and comparing forest-harvest optimisation studies.
