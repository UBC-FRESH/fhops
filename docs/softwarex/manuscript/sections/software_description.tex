% Section 2: Software description
\section{Software description}
\label{sec:software-description}

\textbf{Overview.} FHOPS ships as a Python package backed by Typer-based CLI tooling and a data contract that keeps datasets, solver inputs, and telemetry consistent. The system breaks down into three layers, each callable via CLI (\texttt{fhops.cli.*}) or Python APIs, so users can script the workflow end-to-end or embed modules inside downstream DSS projects:

\begin{enumerate}
  \item \textbf{Data ingest \& scenario contract} – schema + loaders that convert CSV/YAML datasets into typed \texttt{Scenario} objects.
  \item \textbf{Optimisation services} – heuristics (SA/ILS/Tabu), MIP builder, and evaluation harness.
  \item \textbf{Telemetry \& assets} – reproducible logging, benchmark scripts, figure/table generation.
\end{enumerate}

Each layer is consumable via API or CLI, mirroring the dual entry points emphasized in pycity\_scheduling, TSFEL, and contemporary forest-planning DSS (e.g., PRISM, multi-attribute spatial tools) \cite{nguyen2022prism,kuhmaier2010dss}.

Figure~\ref{fig:fhops-prisma-overview} summarises the automation pipeline that underpins this layered design. The PRISMA-style diagram mirrors the WS3 EI manuscript approach: inputs (scenario contract, curated datasets, automation configs) flow through the FHOPS core (ingest, solver/tuning, telemetry) before automation scripts export shared assets that power both the SoftwareX submission and the Sphinx docs. The include lives under \texttt{sections/includes/} so the same source file can be refreshed via \texttt{make assets} along with the other shared snippets.

\input{sections/includes/prisma_overview}

\subsection{Data ingest and scenario contract}
\textbf{Components.}
\begin{itemize}
  \item \texttt{fhops.scenario.contract} – Pydantic-based models encode blocks, machines, corridors, mobilisation jobs, and shift calendars. Each schema element enforces unit-checked payloads, terrain classes, prescription types, and per-shift KPI fields, following the BC-focused constraints described by \citet{lahrsen2022productivity} and \citet{becker2018lidar}.
  \item Dataset loaders (\texttt{fhops.cli.dataset}) – convert CSV/YAML bundles (e.g., TN-147 skyline corridors, TR-122 Roberts Creek blocks, med42 reference dataset) into typed \texttt{Scenario} objects. The loader emits validation reports (`fhops dataset validate`) and canonicalises IDs so solvers and playback routines can consume them deterministically.
  \item Synthetic generator (\texttt{fhops.scenario.synthetic}) – produces stratified scenarios (`fhops synth generate docs/softwarex/assets/data/datasets/synthetic_small --tier small --seed 101 --overwrite`) with configurable harvest-system mixes, blackout biases, and sample counts. These synthetic tiers underpin the scaling experiments and provide shareable exemplars when real datasets remain private.
  \item Hooks for forthcoming BC validation datasets (community forests, skyline corridors, salvage programs). The interface already supports alternative block layouts and mobilisation constraints so companion studies can drop their data in without modifying FHOPS core.
\end{itemize}

\textbf{Dependencies.} Pydantic for schema enforcement, pandas/numpy for IO, PyYAML/pyarrow for CSV/Parquet parsing.

\textbf{Outputs.} Validated \texttt{Scenario} objects plus manifest metadata: harvest-system inventories, road/mobilisation task tables, calendar definitions, and JSON summaries consumed by the benchmark/playback tools. Every ingest run emits a summary JSON (stored under \texttt{docs/softwarex/assets/data/datasets/}) so readers can inspect the exact dataset composition cited in Section~\ref{sec:illustrative-example}.

\subsection{Optimisation services}
\textbf{Components.}
\begin{itemize}
  \item Heuristics under \texttt{fhops.optimization.heuristics} – Simulated Annealing (SA), Iterated Local Search (ILS), and Tabu search implemented with pluggable operator weights and temperature schedules. Each solver exposes Typer entry points (\texttt{fhops bench suite --include-sa --include-ils --include-tabu}) and shares productivity lookup tables parameterised by empirical studies \cite{lahrsen2022productivity,becker2018lidar}.
  \item Tuning harness (\texttt{scripts/run_tuner.py}) – wraps \texttt{scripts/run_tuning_benchmarks.py} to execute random/grid/Bayesian/ILS/Tabu parameter sweeps over multiple scenarios. Outputs Markdown/CSV leaderboard summaries (\texttt{docs/softwarex/assets/data/tuning/}) and logs per-trial telemetry to JSONL. Fast-mode budgets are available via \texttt{FHOPS\_ASSETS\_FAST=1}.
  \item MIP builder (\texttt{fhops.optimization.mip}) – single-level, shift-based Pyomo formulation solved with HiGHS, borrowing structural ideas (capacity coupling, mobilisation sequencing) from classical bilevel/multi-period harvest models \cite{paradis2018bilevel,nelson2003forestmodels}. Provides an exact baseline when heuristics are insufficient.
  \item Playback + KPI generation (\texttt{fhops.evaluation.playback} / \texttt{fhops.evaluation.metrics}) – replays solver output deterministically or with stochastic shocks (downtime, weather, landing congestion) to compute shift/day KPIs, utilisation, and cost metrics.
\end{itemize}

\textbf{Dependencies.} numpy, scipy, Pyomo/HiGHS (exact solver), Typer/Rich for CLI ergonomics, Optuna for Bayesian tuning, pandas for CSV summaries.

\textbf{Outputs.} For each scenario run, FHOPS writes:
\begin{itemize}
  \item \texttt{summary.(csv|json)} with objective, runtime, assignments, and solver metadata (stored under \texttt{docs/softwarex/assets/data/benchmarks/<scenario>/}).
  \item \texttt{telemetry.jsonl} capturing per-iteration metrics, enabling external analysis or playback.
  \item Tuning reports (leaderboards, comparison tables, per-trial telemetry) under \texttt{docs/softwarex/assets/data/tuning/}.
  \item Stochastic playback CSV/Markdown summaries (\texttt{docs/softwarex/assets/data/playback/<scenario>/<solver>/<mode>/}) and cost breakdowns (\texttt{docs/softwarex/assets/data/costing/}).
\end{itemize}
These artifacts feed Section~\ref{sec:illustrative-example} directly and can be regenerated with \texttt{make manuscript-benchmarks}.

\subsection{Telemetry, assets, and automation}
\textbf{Components.}
\begin{itemize}
  \item Telemetry writer (\texttt{fhops.telemetry}) – centralised logging helpers emit JSONL records with input scenario hash, solver parameters, metrics, and pseudo-random seeds. CLI flags such as \texttt{--telemetry-log} or \texttt{--telemetry-s3-prefix} control destinations.
  \item Asset pipelines – \texttt{docs/softwarex/manuscript/scripts/generate_assets.sh} orchestrates dataset summaries, benchmark suites, tuning harness, playback KPI export, costing demo, scaling sweep, and shared snippet rendering. The dedicated \texttt{make manuscript-benchmarks} target wraps this script and appends run metadata (start time, commit, SHA-256 hash) to \texttt{docs/softwarex/assets/benchmark\_runs.log}.
  \item Shared snippets – Markdown/CSV primaries in \texttt{sections/includes/} flow through \texttt{export\_docs\_assets.py} into LaTeX and RST outputs so the manuscript and Sphinx docs reuse identical prose/tables (e.g., motivation narrative, heuristic matrix, benchmark KPIs).
  \item PRISMA diagram – \texttt{scripts/render\_prisma\_diagram.py} compiles the TikZ workflow into PDF/PNG assets under \texttt{docs/softwarex/assets/figures/}, used in both the manuscript and documentation.
\end{itemize}

\textbf{Automation.} The manuscript Makefile exposes:
\begin{itemize}
  \item \texttt{make assets} – runs the full pipeline and refreshes all data/figure artifacts.
  \item \texttt{make manuscript-benchmarks} – regenerates assets, logs reproducibility evidence (commit, runtime, hash).
  \item \texttt{make manuscript-benchmarks-fast} – identical but with \texttt{FHOPS\_ASSETS\_FAST=1} to reduce solver/tuning budgets for quick smoke tests.
  \item \texttt{make pdf} / \texttt{make all} – builds the SoftwareX PDF via \texttt{latexmk}, after optionally running the asset pipeline.
\end{itemize}
These scripted pathways satisfy the SoftwareX reproducibility guidance and ensure the figures/tables referenced throughout the manuscript align with the tagged release (targeting \texttt{v1.0.0-beta1} at submission).
