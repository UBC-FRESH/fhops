% Section 2: Software description
\section{Software description}
\label{sec:software-description}

\textbf{Overview.} FHOPS ships as a Python package backed by Typer-based CLI tooling and a data contract that keeps datasets, solver inputs, and telemetry consistent. The system breaks down into three layers, each callable via CLI (\texttt{fhops.cli.*}) or Python APIs, so users can script the workflow end-to-end or embed modules inside downstream DSS projects:

\begin{enumerate}
  \item \textbf{Data ingest \& scenario contract} – schema + loaders that convert CSV/YAML datasets into typed \texttt{Scenario} objects.
  \item \textbf{Optimisation services} – heuristics (SA/ILS/Tabu), MIP builder, and evaluation harness.
  \item \textbf{Telemetry \& assets} – reproducible logging, benchmark scripts, figure/table generation.
\end{enumerate}

Each layer is consumable via API or CLI, mirroring the dual entry points emphasized in pycity\_scheduling, TSFEL, and contemporary forest-planning DSS (e.g., PRISM, multi-attribute spatial tools) \cite{nguyen2022prism,kuhmaier2010dss}.

Figure~\ref{fig:fhops-prisma-overview} summarises the automation pipeline that underpins this layered design. The PRISMA-style diagram mirrors the WS3 EI manuscript approach: inputs (scenario contract, curated datasets, automation configs) flow through the FHOPS core (ingest, solver/tuning, telemetry) before automation scripts export shared assets that power both the SoftwareX submission and the Sphinx docs. The include lives under \texttt{sections/includes/} so the same source file can be refreshed via \texttt{make assets} along with the other shared snippets.

\input{sections/includes/prisma_overview}

\subsection{Data ingest and scenario contract}
\textbf{Components.}
\begin{itemize}
  \item \texttt{fhops.scenario.contract} – Pydantic-based models encode blocks, machines, corridors, mobilisation jobs, and shift calendars. Each schema element enforces unit-checked payloads, terrain classes, prescription types, and per-shift KPI fields, following the BC-focused constraints described by \citet{lahrsen2022productivity} and \citet{becker2018lidar}.
  \item Dataset loaders (\texttt{fhops.cli.dataset}) – convert CSV/YAML bundles (e.g., TN-147 skyline corridors, TR-122 Roberts Creek blocks, med42 reference dataset) into typed \texttt{Scenario} objects. The loader emits validation reports (`fhops dataset validate`) and canonicalises IDs so solvers and playback routines can consume them deterministically.
  \item Synthetic generator (\texttt{fhops.scenario.synthetic}) – produces stratified scenarios (`fhops synth generate docs/softwarex/assets/data/datasets/synthetic_small --tier small --seed 101 --overwrite`) with configurable harvest-system mixes, blackout biases, and sample counts. These synthetic tiers underpin the scaling experiments and provide shareable exemplars when real datasets remain private.
  \item Hooks for forthcoming BC validation datasets (community forests, skyline corridors, salvage programs). The interface already supports alternative block layouts and mobilisation constraints so companion studies can drop their data in without modifying FHOPS core.
\end{itemize}

\textbf{Dependencies.} Pydantic for schema enforcement, pandas/numpy for IO, PyYAML/pyarrow for CSV/Parquet parsing.

\textbf{Outputs.} Validated \texttt{Scenario} objects plus manifest metadata: harvest-system inventories, road/mobilisation task tables, calendar definitions, and JSON summaries consumed by the benchmark/playback tools. Every ingest run emits a summary JSON (stored under \texttt{docs/softwarex/assets/data/datasets/}) so readers can inspect the exact dataset composition cited in Section~\ref{sec:illustrative-example}.

\subsection{Optimisation services}
\textbf{Components.}
\begin{itemize}
  \item Heuristics under \texttt{fhops.optimization.heuristics} – Simulated Annealing (SA), Iterated Local Search (ILS), and Tabu search implemented with pluggable operator weights and temperature schedules. Each solver exposes Typer entry points (\texttt{fhops bench suite --include-sa --include-ils --include-tabu}) and shares productivity lookup tables parameterised by empirical studies \cite{lahrsen2022productivity,becker2018lidar}.
  \item Tuning harness (\texttt{scripts/run_tuner.py}) – wraps \texttt{scripts/run_tuning_benchmarks.py} to execute random/grid/Bayesian/ILS/Tabu parameter sweeps over multiple scenarios. Outputs Markdown/CSV leaderboard summaries (\texttt{docs/softwarex/assets/data/tuning/}) and logs per-trial telemetry to JSONL. Fast-mode budgets are available via \texttt{FHOPS\_ASSETS\_FAST=1}.
  \item MIP builder (\texttt{fhops.optimization.mip}) – single-level, shift-based Pyomo formulation solved with HiGHS, borrowing structural ideas (capacity coupling, mobilisation sequencing) from classical bilevel/multi-period harvest models \cite{paradis2018bilevel,nelson2003forestmodels}. Provides an exact baseline when heuristics are insufficient.
  \item Playback + KPI generation (\texttt{fhops.evaluation.playback} / \texttt{fhops.evaluation.metrics}) – replays solver output deterministically or with stochastic shocks (downtime, weather, landing congestion) to compute shift/day KPIs, utilisation, and cost metrics.
\end{itemize}

\textbf{Dependencies.} numpy, scipy, Pyomo/HiGHS (exact solver), Typer/Rich for CLI ergonomics, Optuna for Bayesian tuning, pandas for CSV summaries.

\textbf{Outputs.} Each solver/tuning run produces a summary (objective, runtime, assignments), per-iteration telemetry, and optional playback/cost reports through the standard FHOPS CLI. Users control the destination via \texttt{--out-dir} and related flags, so assets can be archived, compared, or re-ingested by downstream tools. These same artifacts underpin the illustrative example in Section~\ref{sec:illustrative-example}.

\subsection{Telemetry, assets, and automation}
\textbf{Components.}
\begin{itemize}
  \item Telemetry writer (\texttt{fhops.telemetry}) – centralised logging helpers emit structured JSONL records with scenario hashes, solver parameters, and metrics. CLI flags such as \texttt{--telemetry-log} or \texttt{--telemetry-s3-prefix} route logs to disk or external stores for audit and replay.
  \item CLI exports – \texttt{fhops.cli.benchmarks}, \texttt{fhops.cli.playback}, and \texttt{fhops.cli.dataset} expose consistent \texttt{--out-dir} semantics so benchmarks, tuning studies, playback summaries, and costing reports can be regenerated on demand (e.g., \texttt{fhops bench suite --scenario ... --include-sa --out-dir bench/minitoy}).
  \item Shared text snippets – Source-of-truth Markdown/CSV assets live alongside the codebase and are converted into both LaTeX and RST fragments, ensuring the SoftwareX manuscript and FHOPS documentation reuse identical descriptions (e.g., motivation narrative, heuristic matrix).
  \item Workflow diagram – Figure~\ref{fig:fhops-prisma-overview} summarises how FHOPS ingests scenario contracts, applies solvers/tuners, and exports telemetry/playback artefacts so readers can see the end-to-end automation path that the CLI exposes.
\end{itemize}

\textbf{Automation.} FHOPS users reproduce the experiments by chaining the CLI commands described above: generate or load a scenario, run \texttt{fhops bench suite} (optionally with \texttt{FHOPS\_ASSETS\_FAST=1} for shorter smoke tests), execute \texttt{scripts/run\_tuner.py}, and replay results with \texttt{fhops playback}. Each command accepts \texttt{--out-dir} and \texttt{--telemetry-log} arguments, so the resulting summaries, telemetry, and KPI tables can be archived or versioned alongside a release tag (e.g., \texttt{v1.0.0-beta1}). This workflow satisfies the SoftwareX reproducibility guidance without requiring any manuscript-specific tooling.
