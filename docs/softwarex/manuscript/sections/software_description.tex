% Section 2: Software description
\section{Software description}
\label{sec:software-description}

\textbf{Overview.} FHOPS ships as a Python package backed by Typer-based CLI tooling and a data contract that keeps datasets, solver inputs, and telemetry consistent. The system breaks down into three layers, each callable via CLI (\texttt{fhops.cli.*}) or Python APIs, so users can script the workflow end-to-end or embed modules inside downstream DSS projects:

\begin{enumerate}
  \item \textbf{Data ingest \& scenario contract} – schema + loaders that convert CSV/YAML datasets into typed \texttt{Scenario} objects.
  \item \textbf{Optimisation services} – heuristics (SA/ILS/Tabu), MIP builder, and evaluation harness.
  \item \textbf{Telemetry \& assets} – reproducible logging, benchmark scripts, figure/table generation.
\end{enumerate}

Each layer is consumable via API or CLI, mirroring the dual entry points emphasized in pycity\_scheduling, TSFEL, and contemporary forest-planning DSS (e.g., PRISM, multi-attribute spatial tools) \cite{nguyen2022prism,kuhmaier2010dss}.

Figure~\ref{fig:fhops-prisma-overview} summarises the automation pipeline that underpins this layered design. The PRISMA-style diagram follows the data from scenario contracts and curated datasets through FHOPS’ ingest, solver, tuning, and telemetry components, then out to benchmark/playback artefacts that readers can reproduce with the same CLI sequence. It visually reinforces that FHOPS treats reproducibility as part of the core platform rather than an add-on.

\input{sections/includes/prisma_overview}

\subsection{Data ingest and scenario contract}
\textbf{Components.}
\begin{itemize}
  \item \texttt{fhops.scenario.contract} – Pydantic-based models encode blocks, machines, corridors, mobilisation jobs, and shift calendars. Each schema element enforces unit-checked payloads, terrain classes, prescription types, and per-shift KPI fields, following the BC-focused constraints described by \citet{lahrsen2022productivity} and \citet{becker2018lidar}.
  \item Dataset loaders (\texttt{fhops.cli.dataset}) – convert CSV/YAML bundles (e.g., TN-147 skyline corridors, TR-122 Roberts Creek blocks, med42 reference dataset) into typed \texttt{Scenario} objects. The loader emits validation reports (`fhops dataset validate`) and canonicalises IDs so solvers and playback routines can consume them deterministically.
  \item Synthetic generator (\texttt{fhops.scenario.synthetic}) – produces stratified scenarios (e.g., \texttt{fhops synth generate out/synthetic\_small --tier small --seed 101 --overwrite}) with configurable harvest-system mixes, blackout biases, and sample counts. These synthetic tiers underpin the scaling experiments and provide shareable exemplars when real datasets remain private.
  \item Hooks for forthcoming BC validation datasets (community forests, skyline corridors, salvage programs). The interface already supports alternative block layouts and mobilisation constraints so companion studies can drop their data in without modifying FHOPS core.
\end{itemize}

\textbf{Dependencies.} Pydantic for schema enforcement, pandas/numpy for IO, PyYAML/pyarrow for CSV/Parquet parsing.

\textbf{Outputs.} Validated \texttt{Scenario} objects plus manifest metadata: harvest-system inventories, road/mobilisation task tables, calendar definitions, and JSON summaries consumed by the benchmark/playback tools. Each ingest run writes a summary JSON to the user-specified output directory so the dataset composition cited in Section~\ref{sec:illustrative-example} can be inspected or versioned.

\subsection{Optimisation services}
\textbf{Components.}
\begin{itemize}
  \item Heuristics under \texttt{fhops.optimization.heuristics} – Simulated Annealing (SA), Iterated Local Search (ILS), and Tabu search implemented with pluggable operator weights and temperature schedules. Each solver exposes Typer entry points (\texttt{fhops bench suite --include-sa --include-ils --include-tabu}) and shares productivity lookup tables parameterised by empirical studies \cite{lahrsen2022productivity,becker2018lidar}.
  \item Tuning harness – FHOPS ships a companion tuning runner that executes random, grid, Bayesian, ILS, and Tabu parameter sweeps across multiple scenarios. It emits leaderboard summaries and telemetry logs so users can compare heuristics with the same instrumentation used for the manuscript benchmarks. Fast-mode budgets are available via \texttt{FHOPS\_ASSETS\_FAST=1}.
  \item MIP builder (\texttt{fhops.optimization.mip}) – single-level, shift-based Pyomo formulation solved with HiGHS, borrowing structural ideas (capacity coupling, mobilisation sequencing) from classical bilevel/multi-period harvest models \cite{paradis2018bilevel,nelson2003forestmodels}. Provides an exact baseline when heuristics are insufficient.
  \item Playback + KPI generation (\texttt{fhops.evaluation.playback} / \texttt{fhops.evaluation.metrics}) – replays solver output deterministically or with stochastic shocks (downtime, weather, landing congestion) to compute shift/day KPIs, utilisation, and cost metrics.
\end{itemize}

\textbf{Dependencies.} numpy, scipy, Pyomo/HiGHS (exact solver), Typer/Rich for CLI ergonomics, Optuna for Bayesian tuning, pandas for CSV summaries.

\textbf{Outputs.} Each solver/tuning run produces a summary (objective, runtime, assignments), per-iteration telemetry, and optional playback/cost reports through the standard FHOPS CLI. Users control the destination via \texttt{--out-dir} and related flags, so assets can be archived, compared, or re-ingested by downstream tools. These same artifacts underpin the illustrative example in Section~\ref{sec:illustrative-example}.

\subsection{Telemetry, assets, and automation}
\textbf{Components.}
\begin{itemize}
  \item Telemetry writer (\texttt{fhops.telemetry}) – centralised logging helpers emit structured JSONL records with scenario hashes, solver parameters, and metrics. CLI flags such as \texttt{--telemetry-log} or \texttt{--telemetry-s3-prefix} route logs to disk or external stores for audit and replay.
  \item CLI exports – \texttt{fhops.cli.benchmarks}, \texttt{fhops.cli.playback}, and \texttt{fhops.cli.dataset} expose consistent \texttt{--out-dir} semantics so benchmarks, tuning studies, playback summaries, and costing reports can be regenerated on demand without bespoke scripting.
  \item Shared automation surface – the same instrumentation that powers FHOPS’ documentation (heuristic matrices, motivation narrative, KPI tables) is available to users through versioned Markdown/CSV assets, ensuring manuscripts, reports, and operational dashboards quote the exact same figures.
  \item Workflow diagram – Figure~\ref{fig:fhops-prisma-overview} summarises how FHOPS ingests scenario contracts, applies solvers/tuners, and exports telemetry/playback artefacts so readers can see the end-to-end automation path that the CLI exposes.
\end{itemize}

\textbf{Automation.} FHOPS users reproduce the experiments by chaining the CLI commands described above: generate or load a scenario, run \texttt{fhops bench suite} (optionally with \texttt{FHOPS\_ASSETS\_FAST=1} for shorter smoke tests), invoke the tuning runner, and replay results with \texttt{fhops playback}. Each command accepts \texttt{--out-dir} and \texttt{--telemetry-log} arguments, so the resulting summaries, telemetry, and KPI tables can be archived or versioned alongside a release tag (e.g., \texttt{v1.0.0-beta1}). This workflow satisfies the reproducibility requirements documented in the metadata tables without relying on manuscript-specific tooling.
