% Section 1: Motivation and significance
\section{Motivation and significance}

Modern forest harvesting optimisation problems mirror the increasing complexity highlighted in decades of operations-research and forest-planning literature:\ numerous studies lament that most tools remain bespoke prototypes, tied to a single case study, or hidden behind commercial black boxes \cite{henkelman1978gpssv,weintraub1996new,heinimann2007forest,buongiorno2003decision,kangas2008decision}. Recent reviews reiterate this: despite progress in vehicle-routing decision support \cite{audy2023planning}, PRISM-like DSS stacks \cite{nguyen2022prism}, and ecosystem-view planning frameworks \cite{labarre2025improve}, practitioners still stitch together spreadsheets, GIS workflows, and siloed simulators \cite{ulvdal2022uncertainty,nelson2003forestmodels}. Every new scenario (e.g., introducing skyline systems, injecting salvage corridors, calibrating productivity models) triggers a bespoke analysis loop.

Rosalia Jaffray’s systematic review of operational-planning models catalogues the practical consequences of that fragmentation \cite{jaffray2025systematic}: (i) fewer than 10\,\% of the 140+ studies surveyed ship reusable code or data; (ii) solver designs are highly site-specific, making it difficult to compare heuristics across coastal skyline, tethered, helicopter, and community-forest ground systems; and (iii) reproducibility is rarely demonstrated, with evaluation scripts or telemetry traces almost never released. The review concludes that BC’s diverse operational contexts—steep coastal skylines, Interior community forests, and emerging salvage programs—require a reusable workflow that can ingest heterogeneous scenario contracts, apply multiple solver families, and publish the resulting telemetry so that follow-on validation studies can build from a stable foundation instead of starting from scratch.

FHOPS targets these persistent gaps by packaging the optimisation heuristics, MIP constructs, scenario contract, and evaluation telemetry we already operate in production into a single, research-friendly stack. Like PyLESA, we state explicit gaps up front—multi-problem workflows, telemetry-backed reproducibility, solver governance, shared datasets—and show how FHOPS compresses the “scoping → benchmarking → deployment” loop for harvesting teams. Our aim is complementary to open-source initiatives in forest planning (e.g., PRISM, lidar-enabled harvest-system selection, productivity syntheses) \cite{nguyen2022prism,becker2018lidar,lahrsen2022productivity}: FHOPS focuses on the operations layer, wrapping tactical/operational problem classes into a reusable pipeline and exposing them as Typer CLI + Python APIs.

The manuscript therefore makes three contributions:
1. **Architecture + data contract** – We formalise the FHOPS architecture (scenario ingestion, optimisation services, telemetry) so planners can reason about model scope and reproduce results outside our internal environment. This mirrors the clarity PyLESA and pycity\_scheduling offer for energy-system modelling.
2. **Benchmark + reproducibility tooling** – We publish a benchmark matrix (scheduling, routing, resource allocation) with scripts that regenerate KPI tables/plots via one command, following the best practices PyDDRBG and cashocs established.
3. **Governance + adoption evidence** – We document the governance model (release cadence, contributor process, validation suite) and provide adoption metrics akin to libxc/MOOSE/GROMACS so reviewers can see FHOPS’ community impact.

By design we keep the SoftwareX contribution focused on the reusable platform. The detailed deployment of FHOPS on multiple British Columbia case studies (e.g., skyline corridors, salvage operations, small tenure planning) is being prepared as separate validation studies, ensuring those analyses retain their own novelty while leveraging the shared tooling documented here. That separation lets FHOPS cite the systematic review’s gaps while reserving the upcoming BC-specific analyses for future domain publications.

Section~\ref{sec:software-description} decomposes the software into ingest, optimisation, and evaluation layers and shows how they interact with the open data catalogue. Section~\ref{sec:illustrative-example} walks through a benchmark suite spanning three optimisation classes and the telemetry/CI hooks that keep it reproducible. Section~\ref{sec:impact} quantifies adoption plus governance processes inspired by libxc/MOOSE, and we close with the roadmap and submission logistics to keep FHOPS aligned with the reproducibility signals SoftwareX emphasises \cite{Blauth2023cashocs}.
