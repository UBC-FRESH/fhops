% Section 3: Illustrative example
\section{Illustrative example}
\label{sec:illustrative-example}

To demonstrate how FHOPS’ ingest/solver/evaluation layers fit together we run a reproducible benchmark suite built around the shipped ground-based ladder—tiny7 (short horizon), small21 (medium horizon), and med42 (long horizon). Large84 introduces two concurrent harvest systems and remains reserved for a future rolling-horizon study, so it is excluded until those results are reproducible. The benchmark bundle encompasses:

\begin{itemize}
  \item Three published reference scenarios (\texttt{examples/tiny7}, \texttt{examples/small21}, \texttt{examples/med42}) that cover increasing horizons, machine fleets, and mobilisation footprints while sharing the same ground-based archetype.
  \item A synthetic three-tier scaling set generated via \texttt{fhops synth generate out/synthetic\_\{small,medium,large\}} so readers can inspect runtime vs. problem size without relying on proprietary data.
  \item Companion tuning, playback, costing, and robustness analyses that mirror the automation pipeline described in Section~\ref{sec:software-description}.
\end{itemize}

\input{sections/includes/cli_pipeline}

\subsection{Benchmark suite (SA/ILS/Tabu)}
We execute \texttt{fhops bench suite} across tiny7, small21, and med42 with SA, ILS, and Tabu heuristics plus optional compare presets (\texttt{--compare-preset diversify --compare-preset mobilisation}). Synthetic-small remains the shareable scaling anchor referenced later. Each run produces:
\begin{itemize}
  \item \texttt{summary.(csv|json)} – objective, runtime, assignment count, solver metadata.
  \item \texttt{telemetry.jsonl} – per-iteration objective traces and operator weights.
  \item \texttt{label.txt} / \texttt{scenario\_path.txt} – provenance for reproducibility.
\end{itemize}

The resulting manuscript table (Table~\ref{tab:benchmarks}) summarises solver performance and runtime. Highlights include:
\begin{itemize}
  \item \textbf{Tiny7}: SA’s default preset reaches 4306.5 objective units in $32.7\,$s with 23 assignments, while ILS squeezes out a slightly better 4349.3 objective in $46.1\,$s (same 23 assignments). Tabu matches the ILS objective but requires $292\,$s, underscoring how diversification presets trade runtime for marginal improvements.
  \item \textbf{Small21}: Serves as the middle rung—SA lands at 15627.6 in $77.9\,$s with 90 assignments, ILS climbs to 15638.1 in $86.0\,$s, and Tabu tops the ladder at 15660.0 in $414.8\,$s. The identical assignment counts highlight that the ladder scales horizon length rather than changing the harvest system archetype.
  \item \textbf{Med42}: ILS delivers the best objective ($-41581.7$) in roughly $182\,$s by completing 213 assignments. SA remains a faster but weaker baseline ($-28271.2$ in $124\,$s), while Tabu’s long-run stress test hits $-46063.7$ in $177\,$s, matching the operational budgets we target for future MIP comparisons.
  \item \textbf{Synthetic-small}: Serves as the deterministic scaling anchor (objective 39.79 across all heuristics) with runtimes between $0.7$ and $12\,$s, providing readers a fully shareable benchmark to validate solver behaviour.
\end{itemize}

\begin{table}[t]
  \centering
  \caption{Solver performance across the reference scenarios (lower objective is better for Med42).}
  \label{tab:benchmarks}
  \input{../../assets/data/tables/solver_performance.tex}
\end{table}
Table~\ref{tab:benchmarks} is generated from \texttt{docs/softwarex/assets/data/tables/solver\_performance.(csv|tex)}, which in turn is derived from the \texttt{summary.csv} files stored under \texttt{docs/softwarex/assets/data/benchmarks/}. Inspecting those directories reveals the exact inputs the manuscript references.

\subsection{Tuning highlights}
The tuning harness runs random/grid/Bayesian/ILS/Tabu studies on the same scenarios with shared telemetry hooks. Outputs include:
\begin{itemize}
  \item \texttt{tuner\_leaderboard*.csv} – best-performing configuration per scenario/tuner.
  \item \texttt{tuner\_comparison*.md} – markdown summaries highlighting operator weights, batch sizes, temperature schedules, and objective improvements.
  \item \texttt{telemetry/runs.jsonl} – raw per-trial metrics for reproducibility.
\end{itemize}

Table~\ref{tab:tuning} reports the salient numbers. On Tiny7 the ILS tuner edges SA’s default by $+42.7$ objective units with $1.7\,$s mean runtime. Small21’s Tabu tuner gains $+10.2$ units in $18.2\,$s, while Med42’s current leader is the random-search configuration (objective $-39725.9$, $+11454.7$ versus SA default) built around a batch size of three and emphasising mobilisation-shake operators. We surface those operator weights because the warm-start MIP plumbing—though fully wired—has yet to yield runtime wins on med42; until rolling-horizon experiments land, the heuristics remain the practical workhorses.

\begin{table}[t]
  \centering
  \caption{Best-performing tuner per scenario (delta measured against SA’s default preset).}
  \label{tab:tuning}
  \input{../../assets/data/tables/tuning_leaderboard.tex}
\end{table}
Like the previous table, Table~\ref{tab:tuning} is sourced from \texttt{docs/softwarex/assets/data/tables/tuning\_leaderboard.(csv|tex)}, making it trivial to verify the numbers against the full tuning outputs in \texttt{docs/softwarex/assets/data/tuning/}.

Both tables reference runs driven by the same solver plumbing described in Section~\ref{sec:software-description}: warm-start MIP hooks are wired into every med42 solve even though they have yet to deliver material wall-clock savings, and the heuristics rely on incremental “move diff” scoring with parallel multi-start execution (batched neighbour evaluation exists but remains GIL-bound for now). These caveats are noted inline so reviewers know which features are production-ready versus actively evolving.

\subsection{Playback robustness and costing}
For each scenario we replay the best SA/ILS assignments deterministically and across 50 stochastic samples (downtime/weather/landing shocks) using \texttt{fhops playback}. Outputs (shift/day CSVs, markdown summaries, metrics JSON) quantify utilisation, downtime, and mobilisation spend so we can, for example, show Tiny7’s deterministic SA replay averaging 0.37 utilisation with \$586 mobilisation cost (\texttt{docs/softwarex/assets/data/playback/tiny7/sa/deterministic/summary.md}) while Med42’s deterministic SA replay sits at 0.56 utilisation with \$10.7k mobilisation spend and the stochastic replay drops to 0.53 utilisation with \$505k accumulated congestion costs (\texttt{docs/softwarex/assets/data/playback/med42/sa/(deterministic|stochastic)/summary.md}). Figure~\ref{fig:playback} visualises these utilisation deltas so readers can quickly see which solver/scenario pairs are most sensitive to shocks.

Costing summaries generated via \texttt{fhops dataset estimate-cost} complement the playback metrics by reporting owning/operating/repair rates per machine role. For example, the Med42 feller-buncher carries a \$246.94/SMH rental rate (\$6.65 per m$^3$ at 41.3 m$^3$/PMH) while the grapple skidder runs \$173.70/SMH (\$4.67/m$^3$) according to \texttt{docs/softwarex/assets/data/costing/cost\_summary.(csv|json)}. These numbers let readers tie KPI swings back to financial impacts with the exact CSV rows we ship.
All of these artefacts are regenerated into \texttt{docs/softwarex/assets/data/playback/} and \texttt{docs/softwarex/assets/data/costing/}, so readers can open the CSV/Markdown files directly. The current release keeps all monetary values in CAD (aligned with the datasets we ship); adding other currencies amounts to supplying alternative machine-rate JSON files rather than changing solver code.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{../../assets/data/playback/utilisation_robustness.png}
  \caption{Deterministic vs stochastic utilisation across scenarios; error bars denote standard deviation across samples. Source files: \texttt{docs/softwarex/assets/data/playback/utilisation\_robustness.(png|pdf)} with raw runs under \texttt{docs/softwarex/assets/data/playback/<scenario>/<solver>/<mode>/}.}
  \label{fig:playback}
\end{figure}

\subsection{Scaling sweep}
Finally, the synthetic sweep utility generates small/medium/large tiers with fixed seeds, benchmarks SA with consistent iteration counts, and plots runtime vs. number of blocks. The accompanying CSV/JSON summarise blocks, machines, assignments, objectives, and runtime for each tier: small (4 blocks, objective $-75{,}040$, runtime $31.7\,$s), medium (8 blocks, objective $-261{,}097$, runtime $83.6\,$s), large (16 blocks, objective $-359{,}225$, runtime $105.4\,$s). Figure~\ref{fig:scaling} reproduces this curve, providing the same ``runtime vs. size'' perspective seen in other reproducible optimisation exemplars such as GROMACS and showing that the synthetic tiers share the ladder’s solver budgets.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{../../assets/data/scaling/runtime_vs_blocks.png}
  \caption{Simulated annealing runtime vs. number of blocks for the synthetic tiers (labels denote tier size). Source files: \texttt{docs/softwarex/assets/data/scaling/runtime\_vs\_blocks.(png|pdf)} with raw metrics in \texttt{docs/softwarex/assets/data/scaling/scaling\_summary.(csv|json)}.}
  \label{fig:scaling}
\end{figure}

\subsection{Figures and tables referenced}
\begin{itemize}
  \item \textbf{Table~1} – Solver performance summary for tiny7, small21, med42, and synthetic-small (direct export of \texttt{fhops bench suite} outputs).
  \item \textbf{Table~2} – Tuning leaderboard highlighting the best configuration per scenario (Tiny7, Small21, Med42, Synthetic-small).
  \item \textbf{Figure~1} – PRISMA workflow (Section~\ref{sec:software-description}) for pipeline overview.
  \item \textbf{Figure~2} – Playback robustness plot (deterministic vs. stochastic utilisation deltas) generated from \texttt{fhops playback}.
  \item \textbf{Figure~3} – Runtime vs. blocks curve from the synthetic sweep, illustrating how SA cost scales with tier size.
\end{itemize}

Each of these artifacts can be regenerated by rerunning the FHOPS CLI sequence described above, satisfying the reproducibility requirements and allowing readers to inspect the underlying CSV/JSON files bundled with the release.

\subsection{Reproducibility log and environment}
All manuscript assets trace back to scripted runs logged in \texttt{docs/softwarex/assets/benchmark\_runs.log}. Each log entry captures the exact CLI command (typically \texttt{scripts/generate\_assets.sh} with explicit solver budgets), FHOPS commit, start/finish time, wall-clock runtime, host metadata, and SHA-256 digests of the generated artefacts. The dataset summaries referenced throughout this section live under \texttt{docs/softwarex/assets/data/datasets/} (\texttt{index.json}, per-scenario summaries, and the synthetic-tier \texttt{scenario.yaml}/\texttt{metadata.yaml} with fixed RNG seeds). Scenario-level benchmark outputs (\texttt{summary.csv/json}, telemetry traces, and assignment CSVs) remain in \texttt{docs/softwarex/assets/data/benchmarks/<slug>/}, while tuning/playback/costing/scaling folders store their respective configurations and CSV/Markdown reports.

Hardware and software context is likewise captured: the current benchmarks run on a dual-socket Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} Gold~6254 system (72 logical cores, 768\,GB RAM) with Ubuntu~24.04, Python~3.12.3, Pyomo~6.9.4, HiGHS~1.11.0, and FHOPS~1.0.0a2. Parallel heuristics/tuners inherit their worker counts from \texttt{scripts/generate\_assets.sh} (e.g., \texttt{--ils-workers 24} on med42) so readers can match runtimes to the published budgets. The auxiliary scripts in \texttt{docs/softwarex/manuscript/scripts/} (\texttt{run\_dataset\_inspection.py}, \texttt{run\_tuning\_benchmarks.py}, \texttt{run\_playback\_analysis.py}, \texttt{run\_costing\_demo.py}, \texttt{run\_synthetic\_sweep.py}) define every preprocessing or analysis step cited in the text, ensuring the repository alone suffices to reproduce the figures and tables without hidden notebooks or ad hoc tooling.

Near-term work (captured in the FHOPS roadmap and aligned with the Jaffray et~al. systematic review) targets rolling-horizon re-solving so the large84 stress case can be included in future revisions, along with real-world BC deployments that will demonstrate the framework on active tenures. Those forthcoming results will plug into the same asset pipeline, ensuring any new datasets or solver traces inherit the reproducibility guarantees shown here.
