% Section 3: Illustrative example
\section{Illustrative example}
\label{sec:illustrative-example}

To demonstrate how FHOPS’ ingest/solver/evaluation layers fit together we run a reproducible benchmark suite encompassing:

\begin{itemize}
  \item Two published reference scenarios (\texttt{examples/minitoy} and \texttt{examples/med42}) that mirror common BC operational contexts (ground-based vs. mixed skyline/tethered systems).
  \item A synthetic three-tier scaling set generated via \texttt{fhops synth generate} (\texttt{docs/softwarex/assets/data/scaling/datasets/synthetic\_\{small,medium,large\}}) so reviewers can inspect runtime vs. problem size without relying on proprietary data.
  \item Companion tuning, playback, costing, and robustness analyses that mirror the automation pipeline described in Section~\ref{sec:software-description}.
\end{itemize}

All experiments are scripted: running \texttt{make manuscript-benchmarks} regenerates the datasets, solvers, tuning studies, playback KPIs, costing summaries, and scaling plots discussed below, and logs the run metadata (commit, runtime, SHA-256 digest) to \texttt{docs/softwarex/assets/benchmark\_runs.log}. We report the assets in situ so readers can inspect or re-run each step.

\subsection{Benchmark suite (SA/ILS/Tabu)}
We execute \texttt{fhops bench suite} across minitoy, med42, and synthetic-small scenarios with SA, ILS, and Tabu heuristics plus optional compare presets (\texttt{--compare-preset diversify --compare-preset mobilisation}). Each scenario directory under \texttt{docs/softwarex/assets/data/benchmarks/<scenario>/} contains:
\begin{itemize}
  \item \texttt{summary.(csv|json)} – objective, runtime, assignment count, solver metadata.
  \item \texttt{telemetry.jsonl} – per-iteration objective traces and operator weights.
  \item \texttt{label.txt} / \texttt{scenario\_path.txt} – provenance for reproducibility.
\end{itemize}

These feeds the manuscript’s benchmark table (Section~\ref{sec:illustrative-example}-Figure~1) summarising solver performance and runtime. For each scenario we highlight:
\begin{itemize}
  \item MiniToy: SA achieves the best objective (15.5) with modest runtime (0.46 s), while Tabu converges fastest (0.09 s) but slightly worse objective (17.0). ILS serves as a stability reference.
  \item Med42: SA’s default preset improves dispatch cost (objective $-631.6$) in $\approx$3 s, ILS finds $-1068.9$ solutions at longer runtimes, and Tabu provides diversification albeit with positive objectives on some runs. These figures illustrate the heuristics’ complementary strengths.
  \item Synthetic tier: ensures the synthetic scaling suite uses identical deterministic seeds as the benchmark table.
\end{itemize}

\subsection{Tuning highlights}
The tuning harness (\texttt{docs/softwarex/manuscript/scripts/run\_tuner.py}) runs random/grid/Bayesian/ILS/Tabu studies on the same scenarios. Outputs live under \texttt{docs/softwarex/assets/data/tuning/}, including:
\begin{itemize}
  \item \texttt{tuner\_leaderboard*.csv} – best-performing configuration per scenario/tuner.
  \item \texttt{tuner\_comparison*.md} – markdown summaries highlighting operator weights, batch sizes, temperature schedules, and objective improvements.
  \item \texttt{telemetry/runs.jsonl} – raw per-trial metrics for reproducibility.
\end{itemize}

We distil these into a table that shows, for example, Bayesian tuning improving minitoy SA from 15.5 to 38.0 (blocked objective for a maximisation variant) and medium42 SA from $-631$ to $\approx -472$ when emphasising mobilisation-shake operators.

\subsection{Playback robustness and costing}
For each scenario we replay the best SA/ILS assignments deterministically and stochastically (downtime/weather/landing shocks) using \texttt{scripts/run\_playback\_analysis.py}. Outputs (shift/day CSVs, markdown summaries, metrics JSON) land in \texttt{docs/softwarex/assets/data/playback/<scenario>/<solver>/<mode>/}. The manuscript references:
\begin{itemize}
  \item Boxplot-ready CSVs for utilisation and productivity, illustrating how stochastic shocks increase variance (particularly on med42’s skyline chains).
  \item Markdown summaries that narrate bottlenecks (e.g., mobilisation-induced delays on med42 when landing congestion is simulated).
\end{itemize}

Costing summaries (\texttt{docs/softwarex/assets/data/costing/cost\_summary.(csv|json)}) generated via \texttt{scripts/run\_costing\_demo.py} complement the playback metrics by reporting owning/operating/repair costs per machine role, letting reviewers tie KPI shifts to cost impacts.

\subsection{Scaling sweep}
Finally, \texttt{scripts/run\_synthetic\_sweep.py} generates synthetic tiers (small/medium/large) with fixed seeds, benchmarks SA with consistent iteration counts, and plots runtime vs. number of blocks (\texttt{docs/softwarex/assets/data/scaling/runtime\_vs\_blocks.png}). The accompanying CSV/JSON summarise blocks, machines, assignments, objectives, and runtime for each tier, enabling a scaling figure (Section~\ref{sec:illustrative-example}-Figure~2) comparable to SoftwareX exemplars such as GROMACS.

\subsection{Figures and tables referenced}
\begin{itemize}
  \item \textbf{Table~1} – Solver performance summary for minitoy, med42, synthetic-small (derived from \texttt{benchmarks/*/summary.csv}).
  \item \textbf{Table~2} – Tuning leaderboard (\texttt{tuning/tuner\_leaderboard.csv}) highlighting best configurations per scenario.
  \item \textbf{Figure~1} – PRISMA workflow (Section~\ref{sec:software-description}) for pipeline overview.
  \item \textbf{Figure~2} – Runtime vs. blocks plot from \texttt{docs/softwarex/assets/data/scaling/runtime\_vs\_blocks.png}.
  \item \textbf{Figure~3} – Playback robustness boxplots (generated from \texttt{playback/**/shift.csv}) showing utilisation variance under stochastic shocks.
\end{itemize}

Each of these artifacts is regenerated verbatim by \texttt{make manuscript-benchmarks}, satisfying the SoftwareX reproducibility requirement and allowing reviewers to inspect the underlying CSV/JSON files.
