% Section 3: Illustrative example
\section{Illustrative example}
\label{sec:illustrative-example}

To demonstrate how FHOPS’ ingest/solver/evaluation layers fit together we run a reproducible benchmark suite encompassing:

\begin{itemize}
  \item Two published reference scenarios (\texttt{examples/minitoy} and \texttt{examples/med42}) that mirror common BC operational contexts (ground-based vs. mixed skyline/tethered systems).
  \item A synthetic three-tier scaling set generated via \texttt{fhops synth generate out/synthetic\_\{small,medium,large\}} so readers can inspect runtime vs. problem size without relying on proprietary data.
  \item Companion tuning, playback, costing, and robustness analyses that mirror the automation pipeline described in Section~\ref{sec:software-description}.
\end{itemize}

All experiments use the standard FHOPS CLI/API sequence (scenario generation or ingestion, \texttt{fhops bench suite}, \texttt{scripts/run\_tuner.py}, \texttt{fhops playback}, \texttt{fhops dataset estimate-cost}, \texttt{scripts/run\_synthetic\_sweep.py}). Run metadata (commands, commit IDs, runtimes, SHA-256 digests) accompanies the tagged release so readers can rerun each step directly.

\subsection{Benchmark suite (SA/ILS/Tabu)}
We execute \texttt{fhops bench suite} across minitoy, med42, and synthetic-small scenarios with SA, ILS, and Tabu heuristics plus optional compare presets (\texttt{--compare-preset diversify --compare-preset mobilisation}). Each run produces:
\begin{itemize}
  \item \texttt{summary.(csv|json)} – objective, runtime, assignment count, solver metadata.
  \item \texttt{telemetry.jsonl} – per-iteration objective traces and operator weights.
  \item \texttt{label.txt} / \texttt{scenario\_path.txt} – provenance for reproducibility.
\end{itemize}

The resulting manuscript table (Table~\ref{tab:benchmarks}) summarises solver performance and runtime. Highlights include:
\begin{itemize}
  \item \textbf{MiniToy} (\texttt{benchmarks/minitoy/summary.csv}): SA’s default preset reaches the best objective (15.5 production units) in $0.46\,$s with 17 assignments, while Tabu converges in $0.09\,$s but stops at 17.0 units. ILS provides a deterministic reference (objective 23.0, eight assignments). These runs showcase how users can choose between quality and turnaround time.
  \item \textbf{Med42}: SA delivers $-631.6$ (lower is better due to cost objective) in roughly $3\,$s, whereas ILS drives the objective down to $-1068.9$ in $1.29\,$s by exploring a broader neighbourhood. Tabu, configured for diversification, finds positive objectives (18.7) that act as stress-test baselines. The benchmark CSVs also report utilisation ratios and mobilisation costs that feed Section~\ref{sec:impact}.
  \item \textbf{Synthetic-small}: Provides a repeatable datapoint for the scaling suite (objective 39.79, runtime $0.25\,$s) so we can show trends without exposing proprietary datasets.
\end{itemize}

\subsection{Tuning highlights}
The tuning harness (\texttt{scripts/run\_tuner.py}) runs random/grid/Bayesian/ILS/Tabu studies on the same scenarios. Outputs include:
\begin{itemize}
  \item \texttt{tuner\_leaderboard*.csv} – best-performing configuration per scenario/tuner.
  \item \texttt{tuner\_comparison*.md} – markdown summaries highlighting operator weights, batch sizes, temperature schedules, and objective improvements.
  \item \texttt{telemetry/runs.jsonl} – raw per-trial metrics for reproducibility.
\end{itemize}

Table~\ref{tab:tuning} reports the salient numbers. For example, Bayesian optimisation pushes the minitoy objective from the SA default of 15.5 to 38.0 by emphasising move/swap proportions, while medium42’s best Bayesian trial reaches $-472.2$ (versus $-631.6$ default) by increasing mobilisation-shake weight and cooling more aggressively. The tuning reports also quantify runtime overheads (all sub-second on minitoy, $\approx$0.9 s on med42), giving readers a sense of the search cost.

\subsection{Playback robustness and costing}
For each scenario we replay the best SA/ILS assignments deterministically and stochastically (downtime/weather/landing shocks) using \texttt{fhops playback}. Outputs (shift/day CSVs, markdown summaries, metrics JSON) quantify day-level utilisation, downtime, and mobilisation cost so we can show, for example, med42’s deterministic SA replay averaging 0.39 utilisation with \$1{,}950 mobilisation cost versus the stochastic replay (50 samples) dropping utilisation to 0.37 while accumulating \$96{,}000 of congestion-induced mobilisation. These metrics drive Figure~\ref{fig:playback} (boxplots of utilisation under shocks).

Costing summaries generated via \texttt{fhops dataset estimate-cost} complement the playback metrics by reporting owning/operating/repair rates per machine role. For example, the med42 feller-buncher carries a \$246.94/SMH rental rate (\$4.25 per m$^3$ at 64.6 m$^3$/PMH), while the grapple skidder runs \$173.70/SMH (\$2.99/m$^3$). These numbers let readers tie KPI swings back to financial impacts.

\subsection{Scaling sweep}
Finally, \texttt{scripts/run\_synthetic\_sweep.py} generates synthetic tiers (small/medium/large) with fixed seeds, benchmarks SA with consistent iteration counts, and plots runtime vs. number of blocks. The accompanying CSV/JSON summarise blocks, machines, assignments, objectives, and runtime for each tier: small (4 blocks, runtime 0.25 s), medium (8 blocks, 0.50 s), large (16 blocks, 1.04 s). Figure~\ref{fig:scaling} reproduces this curve, providing the same \enquote{runtime vs. size} perspective seen in SoftwareX exemplars such as GROMACS.

\subsection{Figures and tables referenced}
\begin{itemize}
  \item \textbf{Table~1} – Solver performance summary for minitoy, med42, synthetic-small (derived from the \texttt{fhops bench suite} outputs).
  \item \textbf{Table~2} – Tuning leaderboard highlighting best configurations per scenario.
  \item \textbf{Figure~1} – PRISMA workflow (Section~\ref{sec:software-description}) for pipeline overview.
  \item \textbf{Figure~2} – Runtime vs. blocks plot from the synthetic sweep.
  \item \textbf{Figure~3} – Playback robustness boxplots (generated from the utilisation metrics output by \texttt{fhops playback}) showing variance under stochastic shocks.
\end{itemize}

Each of these artifacts can be regenerated by rerunning the FHOPS CLI sequence described above, satisfying the SoftwareX reproducibility requirement and allowing reviewers to inspect the underlying CSV/JSON files bundled with the release.
